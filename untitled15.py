# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rlY-FljFzRGpbpE_BE_f1m3rpKtwE1AF
"""

"""
improved_dqn_lunarlander.py

Features:
- Fixed brittle API handling for gym reset/step across gym versions.
- Uses action_size (no hardcoded 4).
- Double DQN toggle.
- Soft/Hard target updates.
- Gradient clipping, learning rate scheduler option.
- Random baseline evaluation.
- Save CSVs with rewards/losses and save model checkpoints.
- Text summary output saved to "training_summary.txt".
- CLI-style parameters at top for quick changes.
"""

import gym
import numpy as np
import random
import collections
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from datetime import datetime
import os

# -----------------------------
# Config / Hyperparams
# -----------------------------
ENV_NAME = "LunarLander-v2"
SEED = 42
EPISODES = 300
GAMMA = 0.99
LR = 5e-4
BATCH_SIZE = 64
MEMORY_SIZE = 100000
EPS_START = 1.0
EPS_END = 0.01
EPS_DECAY = 0.995
TARGET_UPDATE_FREQ = 1000   # in steps if hard update
SOFT_TAU = 1e-3             # for soft updates (if use_soft_update=True)
USE_SOFT_UPDATE = False     # if True uses soft updates, else hard updates every TARGET_UPDATE_FREQ steps
USE_DOUBLE_DQN = True
GRAD_CLIP = 10.0
SAVE_DIR = "dqn_runs"
EVAL_EPISODES = 20
RANDOM_BASELINE_EPISODES = 50

# reproducibility
torch.manual_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)

os.makedirs(SAVE_DIR, exist_ok=True)


# -----------------------------
# Q-Network Architecture
# -----------------------------
class DQN(nn.Module):
    def __init__(self, state_size, action_size, hidden=[128,128]):
        super(DQN, self).__init__()
        layers = []
        in_dim = state_size
        for h in hidden:
            layers.append(nn.Linear(in_dim, h))
            layers.append(nn.ReLU())
            in_dim = h
        layers.append(nn.Linear(in_dim, action_size))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)


# -----------------------------
# Replay Buffer
# -----------------------------
class ReplayBuffer:
    def __init__(self, max_size=100000):
        self.buffer = collections.deque(maxlen=max_size)

    def add(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (np.array(states, dtype=np.float32),
                np.array(actions, dtype=np.int64),
                np.array(rewards, dtype=np.float32),
                np.array(next_states, dtype=np.float32),
                np.array(dones, dtype=np.float32))

    def __len__(self):
        return len(self.buffer)


# -----------------------------
# DQN Agent
# -----------------------------
class Agent:
    def __init__(self, state_size, action_size, device=None):
        self.state_size = state_size
        self.action_size = action_size

        self.gamma = GAMMA
        self.lr = LR
        self.batch_size = BATCH_SIZE
        self.epsilon = EPS_START
        self.epsilon_min = EPS_END
        self.epsilon_decay = EPS_DECAY
        self.target_update_freq = TARGET_UPDATE_FREQ
        self.use_double = USE_DOUBLE_DQN
        self.use_soft_update = USE_SOFT_UPDATE
        self.tau = SOFT_TAU
        self.grad_clip = GRAD_CLIP

        self.device = device or (torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu"))

        # networks
        self.q_network = DQN(state_size, action_size).to(self.device)
        self.target_network = DQN(state_size, action_size).to(self.device)
        self.target_network.load_state_dict(self.q_network.state_dict())

        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.lr)
        # optional scheduler (comment/uncomment if desired)
        # self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.9)

        self.memory = ReplayBuffer(max_size=MEMORY_SIZE)
        self.steps = 0
        self.episodes = 0

    def act(self, state, eval_mode=False):
        """Epsilon-greedy action selection. state can be numpy array."""
        if not eval_mode and random.random() < self.epsilon:
            return random.randrange(self.action_size)

        state_t = torch.from_numpy(np.array(state, dtype=np.float32)).unsqueeze(0).to(self.device)
        with torch.no_grad():
            qvals = self.q_network(state_t)
        return int(torch.argmax(qvals, dim=1).item())

    def soft_update(self, source_net, target_net, tau):
        for target_param, source_param in zip(target_net.parameters(), source_net.parameters()):
            target_param.data.copy_( (1.0 - tau) * target_param.data + tau * source_param.data )

    def train_step(self):
        if len(self.memory) < self.batch_size:
            return None

        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
        states_t = torch.from_numpy(states).to(self.device)
        next_states_t = torch.from_numpy(next_states).to(self.device)
        actions_t = torch.from_numpy(actions).long().to(self.device)
        rewards_t = torch.from_numpy(rewards).to(self.device)
        dones_t = torch.from_numpy(dones).to(self.device)

        # current Q
        q_values = self.q_network(states_t)
        q_value = q_values.gather(1, actions_t.unsqueeze(1)).squeeze(1)

        # compute target
        with torch.no_grad():
            if self.use_double:
                # Double DQN: use q_network to choose best action, target network to estimate its value
                next_q_main = self.q_network(next_states_t)
                next_actions = torch.argmax(next_q_main, dim=1, keepdim=True)
                next_q_target = self.target_network(next_states_t)
                max_next_q = next_q_target.gather(1, next_actions).squeeze(1)
            else:
                next_q_target = self.target_network(next_states_t)
                max_next_q = torch.max(next_q_target, dim=1)[0]

            target = rewards_t + (1.0 - dones_t) * self.gamma * max_next_q

        loss = nn.MSELoss()(q_value, target)

        self.optimizer.zero_grad()
        loss.backward()
        # optional gradient clipping
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), self.grad_clip)
        self.optimizer.step()
        # if using scheduler: self.scheduler.step()

        # update target network
        if self.use_soft_update:
            self.soft_update(self.q_network, self.target_network, self.tau)
        else:
            if self.steps % self.target_update_freq == 0 and self.steps > 0:
                self.target_network.load_state_dict(self.q_network.state_dict())

        # decay epsilon
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

        return loss.item()


# -----------------------------
# Environment helpers (Gym API compatibility)
# -----------------------------
def reset_env(env):
    r = env.reset()
    # gym >=0.26 returns (obs, info)
    if isinstance(r, tuple) or isinstance(r, list):
        return r[0]
    return r

def step_env(env, action):
    out = env.step(action)
    # old: (obs, reward, done, info)
    # new: (obs, reward, terminated, truncated, info)
    if len(out) == 4:
        obs, reward, done, info = out
        return obs, reward, done, info
    elif len(out) == 5:
        obs, reward, terminated, truncated, info = out
        done = terminated or truncated
        return obs, reward, done, info
    else:
        raise RuntimeError("Unexpected env.step() return shape")


# -----------------------------
# Training Loop + utilities
# -----------------------------
def evaluate_policy(agent, env, episodes=10, render=False):
    rewards = []
    for _ in range(episodes):
        state = reset_env(env)
        done = False
        total_r = 0.0
        while not done:
            a = agent.act(state, eval_mode=True)
            state, r, done, _ = step_env(env, a)
            total_r += r
            if render:
                env.render()
        rewards.append(total_r)
    return np.mean(rewards), np.std(rewards)

def random_baseline(env_name, episodes=50, seed=SEED):
    env = gym.make(env_name)
    env.seed(seed)
    rewards = []
    for _ in range(episodes):
        s = reset_env(env)
        done = False
        rsum = 0.0
        while not done:
            a = env.action_space.sample()
            s, r, done, _ = step_env(env, a)
            rsum += r
        rewards.append(rsum)
    env.close()
    return np.mean(rewards), np.std(rewards)

def train_dqn(episodes=EPISODES, env_name=ENV_NAME):
    env = gym.make(env_name)
    # seed environment where possible
    try:
        env.reset(seed=SEED)
    except TypeError:
        pass

    state = reset_env(env)
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n

    agent = Agent(state_size=state_size, action_size=action_size)

    rewards_history = []
    loss_history = []
    best_eval = -float("inf")

    total_steps = 0

    for ep in range(episodes):
        state = reset_env(env)
        done = False
        ep_reward = 0.0
        ep_losses = []

        while not done:
            total_steps += 1
            agent.steps += 1

            action = agent.act(state)
            next_state, reward, done, _ = step_env(env, action)

            agent.memory.add(state, action, reward, next_state, float(done))
            loss = agent.train_step()
            if loss is not None:
                ep_losses.append(loss)

            ep_reward += reward
            state = next_state

        rewards_history.append(ep_reward)
        loss_history.append(np.mean(ep_losses) if ep_losses else 0.0)
        agent.episodes += 1

        # periodic evaluation
        if (ep + 1) % 25 == 0:
            avg_eval, std_eval = evaluate_policy(agent, env, episodes=5)
            print(f"[Ep {ep+1}] Eval mean: {avg_eval:.2f} Â± {std_eval:.2f}")
            if avg_eval > best_eval:
                best_eval = avg_eval
                ckpt_path = os.path.join(SAVE_DIR, f"best_model_ep{ep+1}_eval{avg_eval:.2f}.pt")
                torch.save(agent.q_network.state_dict(), ckpt_path)

        print(f"Episode {ep+1}/{episodes} | Reward: {ep_reward:.2f} | Epsilon: {agent.epsilon:.3f} | Loss: {loss_history[-1]:.4f}")

    env.close()
    # save training artifacts
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    rewards_df = pd.DataFrame({"episode": np.arange(1, len(rewards_history)+1), "reward": rewards_history})
    losses_df = pd.DataFrame({"episode": np.arange(1, len(loss_history)+1), "loss": loss_history})
    rewards_csv = os.path.join(SAVE_DIR, f"rewards_{timestamp}.csv")
    losses_csv = os.path.join(SAVE_DIR, f"losses_{timestamp}.csv")
    rewards_df.to_csv(rewards_csv, index=False)
    losses_df.to_csv(losses_csv, index=False)
    # save final model
    final_model_path = os.path.join(SAVE_DIR, f"final_model_{timestamp}.pt")
    torch.save(agent.q_network.state_dict(), final_model_path)

    # also save simple plots
    try:
        plt.figure()
        plt.plot(rewards_history)
        plt.xlabel("Episode")
        plt.ylabel("Reward")
        plt.title("DQN Training Reward Curve")
        plt.tight_layout()
        plt.savefig(os.path.join(SAVE_DIR, f"reward_curve_{timestamp}.png"))
        plt.close()

        plt.figure()
        plt.plot(loss_history)
        plt.xlabel("Episode")
        plt.ylabel("Loss")
        plt.title("Training Loss Curve")
        plt.tight_layout()
        plt.savefig(os.path.join(SAVE_DIR, f"loss_curve_{timestamp}.png"))
        plt.close()
    except Exception:
        pass

    summary = {
        "final_mean_reward": float(np.mean(rewards_history[-25:])) if len(rewards_history) >= 25 else float(np.mean(rewards_history)),
        "best_eval": float(best_eval),
        "rewards_csv": rewards_csv,
        "losses_csv": losses_csv,
        "final_model": final_model_path
    }

    # write summary text
    summary_txt = os.path.join(SAVE_DIR, f"training_summary_{timestamp}.txt")
    with open(summary_txt, "w") as f:
        f.write("DQN Training Summary\n")
        f.write("====================\n")
        for k, v in summary.items():
            f.write(f"{k}: {v}\n")

    return rewards_history, loss_history, summary


if __name__ == "__main__":
    # compute a simple random baseline first (textual deliverable)
    mean_rand, std_rand = random_baseline(ENV_NAME, episodes=RANDOM_BASELINE_EPISODES)
    print(f"Random baseline over {RANDOM_BASELINE_EPISODES} episodes -> mean: {mean_rand:.2f}, std: {std_rand:.2f}")

    rewards, losses, summary = train_dqn(episodes=EPISODES)
    print("\nTraining complete.")
    print("Summary:")
    for k, v in summary.items():
        print(f"  {k}: {v}")